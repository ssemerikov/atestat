#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–∏–Ω–≥ Excel —Ñ–∞–π–ª—É –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó –ó–í–û

–ê–≤—Ç–æ—Ä–∏: –ì–∞–º–∞–Ω—é–∫ –í—ñ—Ç–∞ –ê–Ω–∞—Ç–æ–ª—ñ—ó–≤–Ω–∞, –°. –û. –°–µ–º–µ—Ä—ñ–∫–æ–≤
–ó–∞—Å—ñ–± —Ä–æ–∑—Ä–æ–±–∫–∏: Claude Code
"""

import pandas as pd
import numpy as np
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')


class AttestationDataParser:
    """–ö–ª–∞—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥—É —Ç–∞ –∫–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—ó –¥–∞–Ω–∏—Ö –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó –ó–í–û"""

    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏ –∑ –º–µ—Ç–æ–¥–∏–∫–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó
    TOTAL_KI = 54.0  # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—É–º–∞ –≤–∞–≥–æ–≤–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ –¥–ª—è –ó–í–û (35 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –¥–ª—è –ù–£ = 52.5)

    # –í–∞–≥–æ–≤—ñ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –≤—Å—ñ—Ö 37 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ (–∑–≥—ñ–¥–Ω–æ –∑ –æ—Ñ—ñ—Ü—ñ–π–Ω–æ—é –º–µ—Ç–æ–¥–∏–∫–æ—é)
    INDICATORS_KI = {
        # –ë–ª–æ–∫ 1: –ö–∞–¥—Ä–æ–≤–∏–π –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª (6,5)
        'I1': 1.0, 'I2': 1.0, 'I3': 1.0, 'I4': 1.0, 'I5': 0.5, 'I6': 1.0,
        # –ë–ª–æ–∫ 2: –§—ñ–Ω–∞–Ω—Å–æ–≤–∞ –¥—ñ—è–ª—å–Ω—ñ—Å—Ç—å (13,5)
        'I7': 1.0, 'I8': 3.0, 'I9': 2.0, 'I10': 1.0, 'I11': 3.0, 'I12': 2.0, 'I13': 1.0, 'I14': 0.5,
        # –ë–ª–æ–∫ 3: –ü—É–±–ª—ñ–∫–∞—Ü—ñ–π–Ω–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å (9,55)
        'I15': 1.5, 'I16': 1.2, 'I17': 1.0, 'I18': 1.0, 'I19': 1.5, 'I20': 0.75, 'I21': 0.35, 'I22': 0.5, 'I23': 0.2,
        # –ë–ª–æ–∫ 4: –Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞ –≤–ª–∞—Å–Ω—ñ—Å—Ç—å (13,5)
        'I24': 1.0, 'I25': 1.0, 'I26': 3.0, 'I27': 1.0, 'I28': 1.0, 'I29': 4.0, 'I30': 0.5, 'I31': 2.0,
        # –ë–ª–æ–∫ 5: –ö–æ–Ω–∫—É—Ä—Å–Ω–µ —Ñ—ñ–Ω–∞–Ω—Å—É–≤–∞–Ω–Ω—è (13,5)
        'I32': 4.0, 'I33': 1.0, 'I34': 2.0, 'I35': 0.5, 'I36': 2.0, 'I37': 4.0
    }

    # –ë–ª–æ–∫–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
    INDICATOR_BLOCKS = {
        '–ö–∞–¥—Ä–æ–≤–∏–π –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª': ['I1', 'I2', 'I3', 'I4', 'I5', 'I6'],
        '–§—ñ–Ω–∞–Ω—Å–æ–≤–∞ –¥—ñ—è–ª—å–Ω—ñ—Å—Ç—å': ['I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13', 'I14'],
        '–ü—É–±–ª—ñ–∫–∞—Ü—ñ–π–Ω–∞ –∞–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å': ['I15', 'I16', 'I17', 'I18', 'I19', 'I20', 'I21', 'I22'],
        '–Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞ –≤–ª–∞—Å–Ω—ñ—Å—Ç—å': ['I23', 'I24', 'I25', 'I26', 'I27', 'I28', 'I29', 'I30', 'I31'],
        '–ö–æ–Ω–∫—É—Ä—Å–Ω–µ —Ñ—ñ–Ω–∞–Ω—Å—É–≤–∞–Ω–Ω—è': ['I32', 'I33', 'I34', 'I35', 'I36', 'I37']
    }

    # –ù–∞—É–∫–æ–≤—ñ –Ω–∞–ø—Ä—è–º–∏
    SCIENCE_DIRECTIONS = {
        1: '–ê–≥—Ä–∞—Ä–Ω–æ-–≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω–∏–π',
        2: '–ì—É–º–∞–Ω—ñ—Ç–∞—Ä–Ω–æ-–º–∏—Å—Ç–µ—Ü—å–∫–∏–π',
        3: '–°—É—Å–ø—ñ–ª—å–Ω–∏–π',
        4: '–ë—ñ–æ–º–µ–¥–∏—á–Ω–∏–π',
        5: '–ü—Ä–∏—Ä–æ–¥–Ω–∏—á–æ-–º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–∏–π',
        6: '–Ü–Ω–∂–µ–Ω–µ—Ä–Ω–æ-—Ç–µ—Ö–Ω–æ–ª–æ–≥—ñ—á–Ω–∏–π',
        7: '–ë–µ–∑–ø–µ–∫–æ–≤–∏–π'
    }

    # –ì—Ä—É–ø–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó
    ATTESTATION_GROUPS = {
        '–ê': {'min': 75, 'max': 100, 'description': '–ù–∞–π–≤–∏—â–∞ –æ—Ü—ñ–Ω–∫–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ'},
        '–ë': {'min': 50, 'max': 75, 'description': '–í–∏—Å–æ–∫–∞ –æ—Ü—ñ–Ω–∫–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ'},
        '–í': {'min': 25, 'max': 50, 'description': '–ó–∞–¥–æ–≤—ñ–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ'},
        '–ì': {'min': 0, 'max': 25, 'description': '–ù–ï –ü–†–û–ô–®–õ–ò –ê–¢–ï–°–¢–ê–¶–Ü–Æ'}
    }

    def __init__(self, excel_path: str):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä—Å–µ—Ä–∞

        Args:
            excel_path: –®–ª—è—Ö –¥–æ Excel —Ñ–∞–π–ª—É –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó
        """
        self.excel_path = Path(excel_path)
        if not self.excel_path.exists():
            raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {excel_path}")

        self.sheets = {}
        self.consolidated_data = None
        self.validation_results = {}

    def load_all_sheets(self) -> Dict[str, pd.DataFrame]:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–∫–ª–∞–¥–æ–∫ Excel —Ñ–∞–π–ª—É

        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ –Ω–∞–∑–≤–∞–º–∏ –≤–∫–ª–∞–¥–æ–∫ —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ DataFrame
        """
        print(f"üìÇ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Excel —Ñ–∞–π–ª—É: {self.excel_path.name}")

        try:
            excel_file = pd.ExcelFile(self.excel_path, engine='openpyxl')
            sheet_names = excel_file.sheet_names

            print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(sheet_names)} –≤–∫–ª–∞–¥–æ–∫:")

            for sheet_name in sheet_names:
                df = pd.read_excel(excel_file, sheet_name=sheet_name)
                self.sheets[sheet_name] = df
                print(f"  ‚úì {sheet_name}: {df.shape[0]} —Ä—è–¥–∫—ñ–≤ √ó {df.shape[1]} —Å—Ç–æ–≤–ø—Ü—ñ–≤")

            return self.sheets

        except Exception as e:
            raise Exception(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ Excel —Ñ–∞–π–ª—É: {str(e)}")

    def _extract_indicator_columns(self, df_raw: pd.DataFrame) -> Dict[str, int]:
        """
        –í–∏—Ç—è–≥—É–≤–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å—ñ–≤ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ I1-I37
        –∑ —Ä—è–¥–∫–∞ 0 (row with indicator labels)

        Args:
            df_raw: –°–∏—Ä–∏–π DataFrame –∑ –≤–∫–ª–∞–¥–∫–∏ –î–æ–≤—ñ–¥–Ω–∏–∫–∏

        Returns:
            –°–ª–æ–≤–Ω–∏–∫ {indicator: column_index}, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥ {'I1': 17, 'I2': 20, ...}
        """
        import re

        indicator_columns = {}

        # –†—è–¥–æ–∫ 0 –º—ñ—Å—Ç–∏—Ç—å –ø–æ–∑–Ω–∞—á–∫–∏ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        header_row = df_raw.iloc[0]

        for col_idx, value in enumerate(header_row):
            if pd.notna(value):
                value_str = str(value).strip()
                # –®—É–∫–∞—î–º–æ –ø–∞—Ç–µ—Ä–Ω "I" followed by digits (–±–µ–∑ –∑—ñ—Ä–æ—á–∫–∏)
                match = re.match(r'^I(\d+)$', value_str)
                if match:
                    indicator_num = match.group(1)
                    indicator_name = f'I{indicator_num}'
                    indicator_columns[indicator_name] = col_idx

        return indicator_columns

    def parse_dovidnyky_sheet(self) -> Tuple[pd.DataFrame, Dict[str, int]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–æ–≤—ñ–¥–Ω–∏–∫–∏' –∑ –≤—Å—ñ–º–∞ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏ P, R, F, I
        –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ä–æ–∑–±–æ—Ä–æ–º –±–∞–≥–∞—Ç–æ—Ä—ñ–≤–Ω–µ–≤–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤

        Returns:
            Tuple –∑ DataFrame (–¥–∞–Ω—ñ, –ø—Ä–æ–ø—É—Å—Ç–∏–≤—à–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏) —Ç–∞ —Å–ª–æ–≤–Ω–∏–∫–æ–º —ñ–Ω–¥–µ–∫—Å—ñ–≤ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        """
        print("\nüìã –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–æ–≤—ñ–¥–Ω–∏–∫–∏'...")

        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–∏—Ä—ñ –¥–∞–Ω—ñ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
        df_raw = pd.read_excel(self.excel_path, sheet_name='–î–æ–≤—ñ–¥–Ω–∏–∫–∏', nrows=5, engine='openpyxl')

        # –í–∏—Ç—è–≥—É—î–º–æ —ñ–Ω–¥–µ–∫—Å–∏ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        indicator_columns = self._extract_indicator_columns(df_raw)

        print(f"  ‚úì –ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ I: {len(indicator_columns)}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –≤—Å—ñ—Ö 37 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        expected_indicators = set(self.INDICATORS_KI.keys())
        found_indicators = set(indicator_columns.keys())
        missing_indicators = expected_indicators - found_indicators

        if missing_indicators:
            print(f"  ‚ö† –í—ñ–¥—Å—É—Ç–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏: {sorted(missing_indicators)}")
        else:
            print(f"  ‚úÖ –ó–Ω–∞–π–¥–µ–Ω–æ –í–°–Ü 37 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤!")

        # –¢–µ–ø–µ—Ä –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ä–µ–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ, –ø—Ä–æ–ø—É—Å–∫–∞—é—á–∏ –ø–µ—Ä—à—ñ 3 —Ä—è–¥–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤
        df_data = pd.read_excel(self.excel_path, sheet_name='–î–æ–≤—ñ–¥–Ω–∏–∫–∏', skiprows=3, engine='openpyxl')

        print(f"  ‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df_data)} –ó–í–û –∑ –¥–∞–Ω–∏–º–∏")

        return df_data, indicator_columns

    def parse_results_sheet(self) -> pd.DataFrame:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–†–µ–∑—É–ª—å—Ç–∞—Ç–∏' –∑ –ø—ñ–¥—Å—É–º–∫–æ–≤–∏–º–∏ –æ—Ü—ñ–Ω–∫–∞–º–∏

        Returns:
            DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó
        """
        print("\nüìä –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–†–µ–∑—É–ª—å—Ç–∞—Ç–∏'...")

        if '–†–µ–∑—É–ª—å—Ç–∞—Ç–∏' not in self.sheets:
            raise KeyError("–í–∫–ª–∞–¥–∫–∞ '–†–µ–∑—É–ª—å—Ç–∞—Ç–∏' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")

        df = self.sheets['–†–µ–∑—É–ª—å—Ç–∞—Ç–∏'].copy()
        print(f"  ‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ –ø—Ä–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó")

        return df

    def parse_detali_sheet(self) -> pd.DataFrame:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–µ—Ç–∞–ª—ñ 3.0' –∑ –¥–µ—Ç–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏

        Returns:
            DataFrame –∑ –¥–µ—Ç–∞–ª—è–º–∏ –ø–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞—Ö
        """
        print("\nüîç –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–µ—Ç–∞–ª—ñ 3.0'...")

        if '–î–µ—Ç–∞–ª—ñ 3.0' not in self.sheets:
            print("  ‚ö† –í–∫–ª–∞–¥–∫–∞ '–î–µ—Ç–∞–ª—ñ 3.0' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return None

        df = self.sheets['–î–µ—Ç–∞–ª—ñ 3.0'].copy()
        print(f"  ‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –¥–µ—Ç–∞–ª—ñ–∑–æ–≤–∞–Ω–∏—Ö –∑–∞–ø–∏—Å—ñ–≤")

        return df

    def parse_dynamika_sheet(self) -> pd.DataFrame:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–∏–Ω–∞–º—ñ–∫–∞' –∑ —á–∞—Å–æ–≤–∏–º–∏ —Ä—è–¥–∞–º–∏ 2019-2023

        Returns:
            DataFrame –∑ –¥–∏–Ω–∞–º—ñ–∫–æ—é –ø–æ–∫–∞–∑–Ω–∏–∫—ñ–≤
        """
        print("\nüìà –ü–∞—Ä—Å–∏–Ω–≥ –≤–∫–ª–∞–¥–∫–∏ '–î–∏–Ω–∞–º—ñ–∫–∞'...")

        if '–î–∏–Ω–∞–º—ñ–∫–∞' not in self.sheets:
            print("  ‚ö† –í–∫–ª–∞–¥–∫–∞ '–î–∏–Ω–∞–º—ñ–∫–∞' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞")
            return None

        df = self.sheets['–î–∏–Ω–∞–º—ñ–∫–∞'].copy()
        print(f"  ‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å—ñ–≤ –¥–∏–Ω–∞–º—ñ–∫–∏")

        return df

    def validate_methodology(self, indicator_columns: Dict[str, int], dovidnyky_df: pd.DataFrame, results_df: pd.DataFrame) -> Dict:
        """
        –í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö –º–µ—Ç–æ–¥–∏—Ü—ñ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó

        Args:
            indicator_columns: –°–ª–æ–≤–Ω–∏–∫ –∑ —ñ–Ω–¥–µ–∫—Å–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            dovidnyky_df: DataFrame –∑ –ø–æ–∫–∞–∑–Ω–∏–∫–∞–º–∏
            results_df: DataFrame –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

        Returns:
            –°–ª–æ–≤–Ω–∏–∫ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        """
        print("\n‚úÖ –í–∞–ª—ñ–¥–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—Å—Ç—ñ –º–µ—Ç–æ–¥–∏—Ü—ñ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó...")

        validation = {
            'total_institutions': len(results_df),
            'total_dovidnyky': len(dovidnyky_df),
            'indicators_found': [],
            'indicators_missing': [],
            'ki_sum_valid': False,
            'formula_valid': False,
            'errors': [],
            'warnings': []
        }

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –≤—Å—ñ—Ö 37 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
        found_indicators = set(indicator_columns.keys())
        expected_indicators = set(self.INDICATORS_KI.keys())

        validation['indicators_found'] = sorted(list(found_indicators))
        validation['indicators_missing'] = sorted(list(expected_indicators - found_indicators))

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—É–º–∏ –≤–∞–≥–æ–≤–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤
        ki_sum = sum(self.INDICATORS_KI.values())
        if abs(ki_sum - self.TOTAL_KI) < 0.01:
            validation['ki_sum_valid'] = True
            print(f"  ‚úì –°—É–º–∞ –≤–∞–≥–æ–≤–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤: {ki_sum:.1f} (–æ—á—ñ–∫—É–≤–∞–ª–æ—Å—å {self.TOTAL_KI})")
        else:
            validation['errors'].append(f"–°—É–º–∞ Ki ({ki_sum:.1f}) –Ω–µ –¥–æ—Ä—ñ–≤–Ω—é—î {self.TOTAL_KI}")

        print(f"  ‚úì –ó–Ω–∞–π–¥–µ–Ω–æ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤: {len(validation['indicators_found'])}/37")

        if validation['indicators_missing']:
            print(f"  ‚ö† –í—ñ–¥—Å—É—Ç–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏: {', '.join(validation['indicators_missing'])}")
        else:
            print(f"  ‚úÖ –í–°–Ü 37 —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –ø—Ä–∏—Å—É—Ç–Ω—ñ –≤ –¥–∞–Ω–∏—Ö!")

        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–æ—Ä–º—É–ª–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ–π–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏ –Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –∑—Ä–∞–∑–∫–∞—Ö
        print("\n  üîç –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–æ—Ä–º—É–ª–∏ –∞—Ç–µ—Å—Ç–∞—Ü—ñ—ó –Ω–∞ –∑—Ä–∞–∑–∫–∞—Ö...")
        self._validate_formula_samples(dovidnyky_df, indicator_columns, validation)

        self.validation_results = validation
        return validation

    def _validate_formula_samples(self, dovidnyky_df: pd.DataFrame, indicator_columns: Dict[str, int], validation: Dict):
        """
        –í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ñ–æ—Ä–º—É–ª–∏ –ê = (–ö + –ï) √ó –†–ü—ñ √ó –ö–†–Ü –Ω–∞ –≤–∏–ø–∞–¥–∫–æ–≤–∏—Ö –∑—Ä–∞–∑–∫–∞—Ö

        Args:
            dovidnyky_df: DataFrame –∑ –¥–∞–Ω–∏–º–∏
            indicator_columns: –°–ª–æ–≤–Ω–∏–∫ —ñ–Ω–¥–µ–∫—Å—ñ–≤ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            validation: –°–ª–æ–≤–Ω–∏–∫ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó –¥–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        """
        # –®—É–∫–∞—î–º–æ –∫–æ–ª–æ–Ω–∫–∏ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        classification_col = None
        expert_col = None
        regional_col = None
        destruction_col = None
        final_col = None

        for col in dovidnyky_df.columns:
            col_str = str(col).lower()
            if '–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ–π–Ω–∞' in col_str and '–æ—Ü—ñ–Ω–∫–∞' in col_str:
                classification_col = col
            elif '–µ–∫—Å–ø–µ—Ä—Ç–Ω–∞' in col_str and '–æ—Ü—ñ–Ω–∫–∞' in col_str:
                expert_col = col
            elif '—Ä–µ–≥—ñ–æ–Ω–∞–ª—å–Ω–∏–π' in col_str and '–∫–æ—î—Ñ—ñ—Ü' in col_str:
                regional_col = col
            elif '—Ä—É–π–Ω—É–≤–∞–Ω—å' in col_str or '—Ä—É–π–Ω—É–≤–∞–Ω–Ω—è' in col_str:
                destruction_col = col
            elif '–∞—Ç–µ—Å—Ç–∞—Ü—ñ–π–Ω–∞' in col_str and '–æ—Ü—ñ–Ω–∫–∞' in col_str:
                final_col = col

        if all([classification_col, expert_col, regional_col, destruction_col, final_col]):
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ñ–æ—Ä–º—É–ª—É –Ω–∞ –∫—ñ–ª—å–∫–æ—Ö –∑—Ä–∞–∑–∫–∞—Ö
            sample_size = min(5, len(dovidnyky_df))
            matches = 0

            for idx in range(sample_size):
                try:
                    K = pd.to_numeric(dovidnyky_df.iloc[idx][classification_col], errors='coerce')
                    E = pd.to_numeric(dovidnyky_df.iloc[idx][expert_col], errors='coerce')
                    RPI = pd.to_numeric(dovidnyky_df.iloc[idx][regional_col], errors='coerce')
                    KRI = pd.to_numeric(dovidnyky_df.iloc[idx][destruction_col], errors='coerce')
                    A_actual = pd.to_numeric(dovidnyky_df.iloc[idx][final_col], errors='coerce')

                    if pd.notna(K) and pd.notna(E) and pd.notna(RPI) and pd.notna(KRI) and pd.notna(A_actual):
                        A_calculated = (K + E) * RPI * KRI

                        # –î–æ–ø—É—Å–∫–∞—î–º–æ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è 0.1%
                        if abs(A_calculated - A_actual) / max(A_actual, 0.01) < 0.001:
                            matches += 1
                except (ValueError, TypeError):
                    continue

            if matches >= sample_size * 0.8:  # 80% —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω—å
                validation['formula_valid'] = True
                print(f"    ‚úÖ –§–æ—Ä–º—É–ª–∞ –≤–∞–ª—ñ–¥–Ω–∞: {matches}/{sample_size} –∑—Ä–∞–∑–∫—ñ–≤ —Å–ø—ñ–≤–ø–∞–ª–∏")
            else:
                validation['warnings'].append(f"–§–æ—Ä–º—É–ª–∞ –Ω–µ –≤–∞–ª—ñ–¥—É—î—Ç—å—Å—è: –ª–∏—à–µ {matches}/{sample_size} –∑—Ä–∞–∑–∫—ñ–≤")
                print(f"    ‚ö† –§–æ—Ä–º—É–ª–∞: {matches}/{sample_size} –∑—Ä–∞–∑–∫—ñ–≤ —Å–ø—ñ–≤–ø–∞–ª–∏")
        else:
            validation['warnings'].append("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—Å—ñ—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ñ–æ—Ä–º—É–ª–∏")

    def consolidate_data(self) -> pd.DataFrame:
        """
        –ö–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—è –≤—Å—ñ—Ö –¥–∞–Ω–∏—Ö —É —î–¥–∏–Ω—É —Å—Ç—Ä—É–∫—Ç—É—Ä—É

        Returns:
            –ö–æ–Ω—Å–æ–ª—ñ–¥–æ–≤–∞–Ω–∏–π DataFrame
        """
        print("\nüîÑ –ö–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö...")

        dovidnyky_df, indicator_columns = self.parse_dovidnyky_sheet()
        results = self.parse_results_sheet()
        detali = self.parse_detali_sheet()
        dynamika = self.parse_dynamika_sheet()

        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–µ—Ç–æ–¥–∏–∫–∏
        self.validate_methodology(indicator_columns, dovidnyky_df, results)

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–∞–ø–ø—ñ–Ω–≥ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –¥–ª—è –ø–æ–¥–∞–ª—å—à–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è
        self.indicator_columns = indicator_columns

        # –û–±'—î–¥–Ω–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–∏—Ö —Ç–∞–±–ª–∏—Ü—å
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç–∞–±–ª–∏—Ü—é –î–æ–≤—ñ–¥–Ω–∏–∫–∏ —è–∫ –æ—Å–Ω–æ–≤—É, –±–æ —Ç–∞–º —î –≤—Å—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        self.consolidated_data = dovidnyky_df

        print(f"  ‚úì –ö–æ–Ω—Å–æ–ª—ñ–¥–æ–≤–∞–Ω–æ {len(self.consolidated_data)} –ó–í–û")

        return self.consolidated_data

    def export_to_csv(self, output_path: str):
        """
        –ï–∫—Å–ø–æ—Ä—Ç –∫–æ–Ω—Å–æ–ª—ñ–¥–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö —É CSV

        Args:
            output_path: –®–ª—è—Ö –¥–æ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª—É
        """
        if self.consolidated_data is None:
            raise ValueError("–°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–æ–Ω–∞–π—Ç–µ –∫–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—é –¥–∞–Ω–∏—Ö")

        print(f"\nüíæ –ï–∫—Å–ø–æ—Ä—Ç —É CSV: {output_path}")

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        self.consolidated_data.to_csv(output_path, index=False, encoding='utf-8-sig')

        print(f"  ‚úì –ó–±–µ—Ä–µ–∂–µ–Ω–æ {len(self.consolidated_data)} –∑–∞–ø–∏—Å—ñ–≤")

    def export_to_json(self, output_dir: str):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è JSON —Ñ–∞–π–ª—ñ–≤ –¥–ª—è –≤–µ–±-–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞

        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è JSON —Ñ–∞–π–ª—ñ–≤
        """
        print(f"\nüì¶ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è JSON —Ñ–∞–π–ª—ñ–≤ —É {output_dir}")

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. –ú–µ—Ç–∞–¥–∞–Ω—ñ –º–µ—Ç–æ–¥–∏–∫–∏
        methodology = {
            'total_ki': self.TOTAL_KI,
            'indicators': self.INDICATORS_KI,
            'blocks': self.INDICATOR_BLOCKS,
            'science_directions': self.SCIENCE_DIRECTIONS,
            'attestation_groups': self.ATTESTATION_GROUPS,
            'indicators_count': len(self.INDICATORS_KI),
            'blocks_count': len(self.INDICATOR_BLOCKS)
        }

        with open(output_dir / 'methodology.json', 'w', encoding='utf-8') as f:
            json.dump(methodology, f, ensure_ascii=False, indent=2)
        print("  ‚úì methodology.json")

        # 2. –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó
        if self.validation_results:
            with open(output_dir / 'validation.json', 'w', encoding='utf-8') as f:
                json.dump(self.validation_results, f, ensure_ascii=False, indent=2)
            print("  ‚úì validation.json")

        # 3. –ö–æ–Ω—Å–æ–ª—ñ–¥–æ–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ (–≤–∏–±—ñ—Ä–∫–∞ –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É)
        if self.consolidated_data is not None:
            # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ DataFrame –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–Ω–∏–∫—ñ–≤
            results_dict = self.consolidated_data.to_dict('records')

            with open(output_dir / 'all_results.json', 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, ensure_ascii=False, indent=2)
            print(f"  ‚úì all_results.json ({len(results_dict)} –∑–∞–ø–∏—Å—ñ–≤)")

        # 4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –Ω–∞—É–∫–æ–≤–∏–º –Ω–∞–ø—Ä—è–º–∞–º
        if '–†–µ–∑—É–ª—å—Ç–∞—Ç–∏' in self.sheets:
            results_df = self.sheets['–†–µ–∑—É–ª—å—Ç–∞—Ç–∏']

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –∫–æ–ª–æ–Ω–∫–∏ –∑ –Ω–∞—É–∫–æ–≤–∏–º –Ω–∞–ø—Ä—è–º–æ–º
            direction_col = None
            for col in results_df.columns:
                if '–Ω–∞–ø—Ä—è–º' in str(col).lower() or 'direction' in str(col).lower():
                    direction_col = col
                    break

            if direction_col:
                stats_by_direction = {}
                for direction_id, direction_name in self.SCIENCE_DIRECTIONS.items():
                    direction_data = results_df[results_df[direction_col] == direction_id]
                    if len(direction_data) > 0:
                        stats_by_direction[direction_name] = {
                            'count': len(direction_data),
                            'direction_id': direction_id
                        }

                with open(output_dir / 'stats_by_direction.json', 'w', encoding='utf-8') as f:
                    json.dump(stats_by_direction, f, ensure_ascii=False, indent=2)
                print("  ‚úì stats_by_direction.json")

        print(f"\n‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è JSON –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {output_dir}")

    def generate_summary_report(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ø—ñ–¥—Å—É–º–∫–æ–≤–æ–≥–æ –∑–≤—ñ—Ç—É

        Returns:
            –¢–µ–∫—Å—Ç –∑–≤—ñ—Ç—É
        """
        report = []
        report.append("=" * 80)
        report.append("–ü–Ü–î–°–£–ú–ö–û–í–ò–ô –ó–í–Ü–¢ –ü–ê–†–°–ò–ù–ì–£ –î–ê–ù–ò–• –ê–¢–ï–°–¢–ê–¶–Ü–á –ó–í–û")
        report.append("=" * 80)
        report.append("")

        report.append(f"üìÇ –í—Ö—ñ–¥–Ω–∏–π —Ñ–∞–π–ª: {self.excel_path.name}")
        report.append(f"üìä –í–∫–ª–∞–¥–æ–∫ –æ–±—Ä–æ–±–ª–µ–Ω–æ: {len(self.sheets)}")
        report.append("")

        if self.validation_results:
            report.append("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–ò –í–ê–õ–Ü–î–ê–¶–Ü–á:")
            report.append(f"  ‚Ä¢ –Ü–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –∑–Ω–∞–π–¥–µ–Ω–æ: {len(self.validation_results['indicators_found'])}/37")
            report.append(f"  ‚Ä¢ –°—É–º–∞ Ki –∫–æ—Ä–µ–∫—Ç–Ω–∞: {self.validation_results['ki_sum_valid']}")

            if self.validation_results['indicators_missing']:
                report.append(f"  ‚ö† –í—ñ–¥—Å—É—Ç–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏: {', '.join(self.validation_results['indicators_missing'])}")

            if self.validation_results['errors']:
                report.append("\n  ‚ùå –ü–û–ú–ò–õ–ö–ò:")
                for error in self.validation_results['errors']:
                    report.append(f"    - {error}")

            if self.validation_results['warnings']:
                report.append("\n  ‚ö† –ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø:")
                for warning in self.validation_results['warnings']:
                    report.append(f"    - {warning}")

        report.append("")
        report.append("=" * 80)

        return "\n".join(report)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""

    # –®–ª—è—Ö–∏ –¥–æ —Ñ–∞–π–ª—ñ–≤
    base_dir = Path(__file__).parent.parent
    excel_path = base_dir / "data" / "–û–≥–æ–ª–æ—à–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.xlsx"
    csv_output = base_dir / "data" / "consolidated_data.csv"
    json_output_dir = base_dir / "data" / "json"

    print("üöÄ –ü–û–ß–ê–¢–û–ö –ü–ê–†–°–ò–ù–ì–£ –î–ê–ù–ò–• –ê–¢–ï–°–¢–ê–¶–Ü–á –ó–í–û")
    print("=" * 80)

    try:
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–∞—Ä—Å–µ—Ä–∞
        parser = AttestationDataParser(str(excel_path))

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –≤–∫–ª–∞–¥–æ–∫
        parser.load_all_sheets()

        # –ö–æ–Ω—Å–æ–ª—ñ–¥–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
        parser.consolidate_data()

        # –ï–∫—Å–ø–æ—Ä—Ç —É CSV
        parser.export_to_csv(str(csv_output))

        # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è JSON —Ñ–∞–π–ª—ñ–≤
        parser.export_to_json(str(json_output_dir))

        # –ü—ñ–¥—Å—É–º–∫–æ–≤–∏–π –∑–≤—ñ—Ç
        print("\n" + parser.generate_summary_report())

        print("\n‚úÖ –ü–ê–†–°–ò–ù–ì –£–°–ü–Ü–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")

    except Exception as e:
        print(f"\n‚ùå –ü–û–ú–ò–õ–ö–ê: {str(e)}")
        raise


if __name__ == "__main__":
    main()
